{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"music-genre-classification","text":"<p>A pipeline for training a neural network for music genre classification.</p> <p>See the Project Summary</p>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>python3</li> <li>ffmpeg (for converting files)</li> <li>make (for ease of use)</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>A makefile provides various utilities to install and run the program.</p> <p><code>make venv</code> creates a virtual environment and installs all requirements</p> <p><code>make docs-server</code> starts the docs server</p>"},{"location":"#usage","title":"Usage","text":"<p>This project is best used as a pyhton package. This is demonstrated in the jupyter notebooks under <code>./experiments/</code>. The output of these notebooks can also be viewed on the docs page.</p>"},{"location":"final_report/","title":"Final Report","text":""},{"location":"final_report/#introduction","title":"Introduction","text":"<p>Labeling a song with a correct genre is an inherently difficult problem. Notably because most often there is no correct answer. Genres blend into each other, there is no clear line of separation and often multiple answers seem to fit. Especially if the genres are closely related subgenres.</p>"},{"location":"final_report/#initial-idea","title":"Initial idea","text":"<p>This led to the idea of using multiple genres for describing a single song to sort of create a spectrum of where the song lies on. Then I would employ deep learning to create a model for classifying songs by a set of genres. This proved to be a lot more difficult than expected. Initially I found rateyourmusic.com, a site where users rate music and which often lists multiple genres for a song.</p> <p>However one major problem with this data source was there was is API. Luckily someone wrote a scrapper for it. While getting ratelimited a lot it worked fine for about 300 songs, but when I decided to try a different approach to reduce rate limiting I got IP banned instantly.</p> <p>Analysing what I had so far it was clear that this wasn't a good route anyway. Only a small fraction of songs even had genres. </p>"},{"location":"final_report/#solution","title":"Solution","text":"<p>The genre information by the spotify api was also not very good. The genres were too generic for my task. So I decided to get the genre data from genre-specific playlists the songs appear in. I did that by selecting 17 playlists that each contain songs of a single genre and then labeling each song by that genre.</p> <p>The downloading of songs was automated by a tool I wrote that searches for and then downloads each song on YouTube.</p>"},{"location":"final_report/#neural-network","title":"Neural Network","text":"<p>Elbir, A., &amp; Aydin, N. [1] present a network for music genre classification. They achieved an accuracy of 81.8% using a CNN and even 97.6% employing an additional SVM on a feature vector from one of the dense layers. I only use a CNN for classification.</p> <p>The best model I settled on was an adaptation of the model from [1]. I added more convolutional layers to create a deeper network. And also added batch normalization which improved the performance.</p> <p>Songs are converted to 16k .wav files for lower storage usage to allow me to load the entire dataset into ram. This improved the training time by multiple orders of magnitude versus loading songs on the fly from my ssd.</p>"},{"location":"final_report/#dataset","title":"Dataset","text":"<p>A mel spectrogram is a spectrogram using a logarithmic scale on the frequency axis. This scaling more similarly approximates human hearing. Such a spectrogram is created for each song using 128 frequency bins. And a window length of 2048 samples for the fourier transform and 75% window overlap.</p> <p>The dataset contains 1280 songs in 8 different genres. As an augmentation step only a subinterval of the whole song is used as an input. This is done by using a 128x128 pixel crop as the model input. With the configuration described above we such a crop contains about 4.2 seconds of a song.</p> <p>In each epoch each song is sampled multiple times, effectively increasing the dataset size.</p>"},{"location":"final_report/#demo","title":"Demo","text":"Your browser does not support the video tag.  <p>The demo app is a simple site build with html, js and picocss. The backend is built on flask mostly using existing code for processing songs and executing the model. The model is downloaded from a github release. Also a docker image is built using github actions and available at <code>ghcr.io/georgschenzel/music-genre-classification:latest</code>. To try it for yourself simply do: <pre><code>$ docker pull ghcr.io/georgschenzel/music-genre-classification:latest\n$ docker run -d -p 8000:8000 ghcr.io/georgschenzel/music-genre-classification:latest\n</code></pre> And go to <code>http://127.0.0.1:8000/</code> in your browser.</p>"},{"location":"final_report/#results","title":"Results","text":"<p>The results are better than expected. The best test accuracy achieved was 84.4%. The test accuracy is determined by sampling whole songs and getting an average of the prediction. Thus achieving a better accuracy than in training or validation as here the network only sees a few seconds of the song.</p> <p></p> <p>The results of the best run. In the confusion matrix you can see a good representation of the model's performance with only a few outliers. \"DnB\" and \"Liquid DnB\" were deliberately chosen as two very similar genres, mainly differing in energy but having very similar drum patterns and bpm. The model hat no problem differentiating between them. A weight decay of 0.005 and a learning rate of 0.0001 was used.</p>"},{"location":"final_report/#takeaways","title":"Takeaways","text":"<ul> <li>SGD performed slightly better then adam</li> <li>Using a much deeper network such as ResNet18 resulted in very strong overfitting</li> <li>Data loading takes long and can bottleneck. Keeping all data in RAM fixed this. For larger datasets one could preprocess all songs into 5s segments and store them instead of loading the full song.</li> <li>Getting accurate genre information for songs is proves to be difficult</li> </ul>"},{"location":"final_report/#what-i-would-do-differently","title":"What I would do differently","text":"<p>I would aim to get a significantly larger dataset. Maybe about 20k songs across 20 genres would be my target. Some optimizations would be needed for such a large dataset to not bottleneck the training speed.</p> <p>Additionally, I would try to sample a much larger amount of playlists, or even automate the process of searching for playlists. This way I could provide multiple genres for each song allowing me to work on my initial idea of placing songs on more of a spectrum than single genre labels.</p>"},{"location":"final_report/#time-spent","title":"Time Spent","text":"Actual Estimate Task 35h 30h Dataset collection 10h - Creating the pipeline 8h 5h Designing and building the network 10h 10h Training and fine-tuning 18h 10h Building the demo 81h 55h Total for project + demo"},{"location":"final_report/#references","title":"References","text":"<p>[1] Elbir, A., &amp; Aydin, N. (2020). Music genre classification and music recommendation by using deep learning. Electronics Letters, 56(12), 627-629.</p>"},{"location":"project_summary/","title":"Project Summary","text":""},{"location":"project_summary/#introduction","title":"Introduction","text":"<p>Labeling a song with a correct genre is an inherntly difficult problem. Notably because most often there is no correct answer. Genres blend into each other, there is no clear line of separation and often multiple answers seem to fit. Especially if the genres are closely related subgenres.</p> <p>This led to the idea of using multiple genres for describing a single song to sortof create a spectrum of where the song lie on. Then I would employ deep learning to create a model for classifying songs by a set of gernes.</p> <p>This proved to be a lot more difficult than expected. Initially I found rateyourmusic.com, a site where users rate music and which often lists multiple genres for a song. Perfect. So I started hacking...</p>"},{"location":"project_summary/#targets","title":"Targets","text":"<p>Elbir, A., &amp; Aydin, N. [1] present a network for music genre classification. They achieved an accuarcy of 81.8% using a CNN and even 97.6% employing an additional SVM on a feature vector from one of the dense layers. I will only do use a CNN for classification. As my choice of genre cover a smaller spectrum of genres that are more simiar to each other. Thus I was aiming for a 60% accuracy.</p>"},{"location":"project_summary/#data-aquisition","title":"Data aquisition","text":"<p>I already had a side project for creating a music collection for djaying. It works by reading in a spotify playlist, searching for the songs on youtube, giving me a preview and then downloading and converting them. So I usesd this tool for my dataset creation, downloading was no problem and already proven working (a bit unstable but after this project it improved a lot). I extended it to also get metadata from rateyourmusic.com and store it in a simple <code>.json</code> file. This is where I was met with the biggest hurdle. There was no API, luckily someone wrote a scrapper for it. While getting ratelimited a lot it worked fine for about 300 songs. When I decided to try a different approch to reduce rate limiting I got instantly IP banned.</p> <p>Analysing what I had so far it was clear that this wasn't a good route anyway.</p> <p> </p> <p>Only a small fraction of songs even had genres. Also spotify did not have a very good selection of genres. So I shifted to the next idea: Get a few genre specific playlists and assign the genres from there. This way I could easiyl create a dataset of 2096 songs.</p> <p></p> <p>The quality of my dataset now depends on whoever created these playlists. But skipping through a the songs it seemed accurate (at least as accurate as you even can describe genres).</p>"},{"location":"project_summary/#the-pipeline","title":"The pipeline","text":"<p>In <code>mglcass</code> I implemented a robust pipeline for colellecting and processing the raw data, creating datasets and training and evaulating different networks.</p>"},{"location":"project_summary/#dataset","title":"Dataset","text":"<p>The dataset contains 2096 songs of 8 genres. For performance reasosns the songs have been converted to 16k wav files. Reading straight from the mp3 files was a huge bottleneck. From the waveforms I create mel spectrograms of the whole songs as a preprocessing step and store all of them in memory.</p> <p>Then a 128x128 image is cropped from the whole spectrogram. Note that the height contains all the frequency bins so only the time dimension is cropped. Using a width of 128 gives, depending one the spectrogram parameters, a window of about 2 to 8 seconds when choosing reasonable values.</p> <p>Using this augmentation step we can increase the effective dataset size by a constant factor. For that the whole dataset is iterated over multiple times in a single epoch.</p>"},{"location":"project_summary/#models","title":"Models","text":"<p>Three different models were employed:</p> <p>MusicRecNet: The model from [1] using 3 convoultional layers, 2 fully connected layers and dropout layers inbetween.</p> <p>ResNet18: As a comparison.</p> <p>MgcNet: My own adaption of <code>MusicRecNet</code> with more convultion layers added. I created this to see what happens with a slightly deeper network.</p>"},{"location":"project_summary/#training","title":"Training","text":"<p>The training was done in jupyter notebooks for better presentation. Here you can see the baisc program usage in a dry-run example. Some of my experiments include a comparison between adam and sgd optimizers. Where sgd had a slight advantage. There are also training runs with ResNet, ResNet + weight decay and MgcNet. Then I performed some bigger training using more epochs and augmentation and parameters from my previous finding. Mainly weight decay was added. Finally, I ran a 100 epoch training using larger weight decay and lower learning rate on my own network where I achieved a 82% accuracy with very little overfitting.</p>"},{"location":"project_summary/#results","title":"Results","text":"<p>The results are better than expected. The best test accuracy achieved was 84.4% using my own model. The test accuracy is determined by sampling whole songs and getting an average of the prediction. Thus achieving a better accuracy than in training or validation as here the network only sees a few seconds of the song.</p> <p> The results of the best run. In the confusion matrix you can see a good representation of the models performance with only a few outliers. \"DnB\" and \"Liqduid DnB\" were deliberately chosen as two very similar genres, mainly differing in energy but having very similar drum patterns and bpm. Between those two classes the model hat the most trouble classifying but still got the majority correct. This run used a weight decay of 0.005 and a learning rate of 0.0001.</p>"},{"location":"project_summary/#conclusion","title":"Conclusion","text":"<p>As the creation of a big and accurate dataset proves to be a difficult task, maybe looking into unsuperviced approaches for clustering songs by genre might be an interesting approach to look into.</p> <p>Another approach I might explore in the future would be to try sampling a much greater number of playlist allowing for finding multiple labels for individual songs.</p>"},{"location":"project_summary/#references","title":"References","text":"<p>[1] Elbir, A., &amp; Aydin, N. (2020). Music genre classification and music recommendation by using deep learning. Electronics Letters, 56(12), 627-629.</p>"},{"location":"experiments/0_data/","title":"Data","text":"<pre><code>%reload_ext autoreload\n%autoreload 2\nfrom mgclass import analysis, MusicGenreDataset\nfrom pathlib import Path\nfrom mgclass.utils import *\nfrom mgclass.raw_data import download_playlists\nfrom mgclass.analysis import plot_spectrogram\n</code></pre> <pre><code>data_path = Path(\"/home/georg/Music/ADL/spotdj.json\")\n</code></pre>"},{"location":"experiments/0_data/#summary-of-the-raw-data","title":"Summary of the raw data","text":"<pre><code>analysis.summarize_spotdj_database(data_path)\n</code></pre> <pre><code>Dataset size: 2096\nWith genres: 265\nWith genres (incl. album): 295\n</code></pre> <p>Dataset using spotify genres</p> <pre><code>dataset = MusicGenreDataset(\ndata_dir=Path(\"/home/georg/Music/ADL/\"),\npreprocess=create_spectrogram(),\nfile_transform=mp3_to_wav_location,\nmax_frames=2048\n)\nanalysis.summarize_dataset(dataset)\n</code></pre> <pre><code>Using most 10 occurring genres from spotify api\n\n\n\nCreating dataset:   0%|          | 0/658 [00:00&lt;?, ?it/s]\n\n\nDataset creation finished in: 5.4624 seconds\n</code></pre> <p></p> <p>Dataset using genre from the playlists songs originate from.</p> <pre><code>dataset = MusicGenreDataset(\ndata_dir=Path(\"/home/georg/Music/ADL/\"),\npreprocess=create_spectrogram(),\nfile_transform=mp3_to_wav_location,\nplaylist_to_genre=sample_playlist_to_genre,\nmax_frames=2048,\neven_classes=False\n)\nanalysis.summarize_dataset(dataset)\n</code></pre> <pre><code>Using genre from playlist source\nDups for 0-6:   1\nDups for 0-7:   1\nDups for 1-4:  10\nDups for 2-7:   3\nDups for 5-7:   1\nPreprocessing complete\n\n\n\nCreating dataset:   0%|          | 0/1559 [00:00&lt;?, ?it/s]\n\n\nDataset creation finished in: 5.7989 seconds\n</code></pre> <p></p> <pre><code>\n</code></pre>"},{"location":"experiments/1_training/","title":"Training Dry Run","text":"<pre><code>%reload_ext autoreload\n%autoreload 2\nfrom pathlib import Path\nfrom mgclass import analysis, MusicGenreDataset, networks\nfrom mgclass.utils import *\nfrom mgclass.training import TrainingRun\n</code></pre> <pre><code>dry_run = False\n</code></pre> <pre><code>dataset = MusicGenreDataset(\ndata_dir=Path(\"/home/georg/Music/ADL/\"),\npreprocess=create_spectrogram(),\nfile_transform=mp3_to_wav_location,\nplaylist_to_genre=sample_playlist_to_genre,\nmax_frames=16000 * 5,\ndry_run = dry_run\n)\nanalysis.summarize_dataset(dataset)\n</code></pre> <pre><code>Using genre from playlist source\nClamping dataset to 160 songs per class. Removing 279 songs.\nPreprocessing complete\n\n\n\n\nCreating dataset:   0%|          | 0/1280 [00:00&lt;?, ?it/s]\n\n\n\n\nDataset creation finished in: 7.1959 seconds\n</code></pre> <pre><code>model = networks.ResNet(dataset.num_classes)\nrun = TrainingRun(dataset, model, epochs=4, dry_run=dry_run, repeat_count=1)\nrun.start()\nrun.test()\nrun.plot()\n</code></pre> <pre><code>  0%|          | 0.00/4.00 [00:00&lt;?, ?epochs/s]\n\n\nStarting training for 4 epochs\nEpoch   1/4, train_loss: 2.077, train_acc: 0.149, val_loss: 2.067, val_acc: 0.211, in 2.45s\nEpoch   2/4, train_loss: 2.062, train_acc: 0.232, val_loss: 2.031, val_acc: 0.258, in 2.04s\nEpoch   3/4, train_loss: 2.044, train_acc: 0.247, val_loss: 2.013, val_acc: 0.250, in 1.92s\nEpoch   4/4, train_loss: 2.030, train_acc: 0.243, val_loss: 2.003, val_acc: 0.266, in 1.84s\nTraining finished in: 8.2517 seconds\n\ntest_loss: 2.053, test_acc: 0.188\n</code></pre> <pre><code>run.plot()\n</code></pre> <pre><code>\n</code></pre>"},{"location":"experiments/2_musicrecnet_adam/","title":"2 musicrecnet adam","text":"<pre><code>%reload_ext autoreload\n%autoreload 2\nfrom pathlib import Path\nfrom mgclass import analysis, MusicGenreDataset, networks\nfrom mgclass.utils import *\nfrom mgclass.training import TrainingRun\n</code></pre> <pre><code>dry_run = False\nepochs = 10\nrepeat_count = 100\n</code></pre> <pre><code>dataset = MusicGenreDataset(\ndata_dir=Path(\"/home/georg/Music/ADL/\"),\npreprocess=create_spectrogram(n_mels=128, win_length=1024),\nfile_transform=mp3_to_wav_location,\nplaylist_to_genre=sample_playlist_to_genre,\ntransform=create_crop((128, 128)),\ndry_run = dry_run,\n#max_frames=16000*60*1\n)\nanalysis.summarize_dataset(dataset)\n</code></pre> <pre><code>Using genre from playlist source\nDups for 0-6:   1\nDups for 0-7:   1\nDups for 1-4:  10\nDups for 2-7:   3\nDups for 5-7:   1\nClamping dataset to 160 songs per class. Removing 279 songs.\nPreprocessing complete\n\n\n\nCreating dataset:   0%|          | 0/1280 [00:00&lt;?, ?it/s]\n\n\nDataset creation finished in: 106.8165 seconds\n</code></pre> <pre><code>model = networks.MusicRecNet(dataset.num_classes)\nrun = TrainingRun(dataset, model, batch_size=128, epochs=epochs, dry_run=dry_run, repeat_count=repeat_count)\nrun.start()\nrun.test()\nrun.plot()\n</code></pre> <pre><code>Starting training for 10 epoch\n\n\n\n  0%|          | 0.00/10.0 [00:00&lt;?, ?epochs/s]\n\n\nEpoch   1/10, train_loss: 1.901, train_acc: 0.389, val_loss: 1.787, val_acc: 0.511, in 54.05s\nEpoch   2/10, train_loss: 1.752, train_acc: 0.544, val_loss: 1.703, val_acc: 0.587, in 51.18s\nEpoch   3/10, train_loss: 1.692, train_acc: 0.599, val_loss: 1.674, val_acc: 0.611, in 51.70s\nEpoch   4/10, train_loss: 1.665, train_acc: 0.624, val_loss: 1.654, val_acc: 0.632, in 50.44s\nEpoch   5/10, train_loss: 1.641, train_acc: 0.645, val_loss: 1.645, val_acc: 0.636, in 52.05s\nEpoch   6/10, train_loss: 1.626, train_acc: 0.659, val_loss: 1.634, val_acc: 0.647, in 51.47s\nEpoch   7/10, train_loss: 1.613, train_acc: 0.671, val_loss: 1.627, val_acc: 0.653, in 51.68s\nEpoch   8/10, train_loss: 1.602, train_acc: 0.680, val_loss: 1.617, val_acc: 0.662, in 51.53s\nEpoch   9/10, train_loss: 1.591, train_acc: 0.691, val_loss: 1.618, val_acc: 0.661, in 52.79s\nEpoch  10/10, train_loss: 1.581, train_acc: 0.700, val_loss: 1.620, val_acc: 0.659, in 51.71s\nTraining finished in: 518.6278 seconds\ntest_loss: 1.658, test_acc: 0.766\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"experiments/2_musicrecnet_sgd/","title":"MusicRecNet","text":"<pre><code>%reload_ext autoreload\n%autoreload 2\nfrom pathlib import Path\nfrom mgclass import analysis, MusicGenreDataset, networks\nfrom mgclass.utils import *\nfrom mgclass.training import TrainingRun\n</code></pre> <pre><code>dry_run = False\nepochs = 10\nrepeat_count = 100\n</code></pre> <pre><code>dataset = MusicGenreDataset(\ndata_dir=Path(\"/home/georg/Music/ADL/\"),\npreprocess=create_spectrogram(n_mels=128, win_length=1024),\nfile_transform=mp3_to_wav_location,\nplaylist_to_genre=sample_playlist_to_genre,\ntransform=create_crop((128, 128)),\ndry_run = dry_run,\n#max_frames=16000*60*1\n)\nanalysis.summarize_dataset(dataset)\n</code></pre> <pre><code>Using genre from playlist source\nDups for 0-6:   1\nDups for 0-7:   1\nDups for 1-4:  10\nDups for 2-7:   3\nDups for 5-7:   1\nClamping dataset to 160 songs per class. Removing 279 songs.\nPreprocessing complete\n\n\n\nCreating dataset:   0%|          | 0/1280 [00:00&lt;?, ?it/s]\n\n\nDataset creation finished in: 112.2773 seconds\n</code></pre> <pre><code>model = networks.MusicRecNet(dataset.num_classes)\nrun = TrainingRun(dataset, model, batch_size=128, epochs=epochs, dry_run=dry_run, repeat_count=repeat_count)\nrun.start()\nrun.test()\nrun.plot()\n</code></pre> <pre><code>Starting training for 10 epoch\n\n\n\n  0%|          | 0.00/10.0 [00:00&lt;?, ?epochs/s]\n\n\nEpoch   1/10, train_loss: 1.976, train_acc: 0.311, val_loss: 1.927, val_acc: 0.365, in 57.03s\nEpoch   2/10, train_loss: 1.880, train_acc: 0.416, val_loss: 1.856, val_acc: 0.431, in 52.44s\nEpoch   3/10, train_loss: 1.829, train_acc: 0.465, val_loss: 1.810, val_acc: 0.475, in 51.53s\nEpoch   4/10, train_loss: 1.790, train_acc: 0.502, val_loss: 1.770, val_acc: 0.520, in 52.12s\nEpoch   5/10, train_loss: 1.760, train_acc: 0.534, val_loss: 1.768, val_acc: 0.507, in 52.28s\nEpoch   6/10, train_loss: 1.735, train_acc: 0.559, val_loss: 1.742, val_acc: 0.538, in 51.05s\nEpoch   7/10, train_loss: 1.716, train_acc: 0.576, val_loss: 1.699, val_acc: 0.585, in 52.73s\nEpoch   8/10, train_loss: 1.700, train_acc: 0.590, val_loss: 1.697, val_acc: 0.584, in 50.40s\nEpoch   9/10, train_loss: 1.694, train_acc: 0.594, val_loss: 1.679, val_acc: 0.604, in 50.64s\nEpoch  10/10, train_loss: 1.677, train_acc: 0.610, val_loss: 1.688, val_acc: 0.590, in 49.62s\nTraining finished in: 519.8487 seconds\ntest_loss: 1.668, test_acc: 0.812\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"experiments/2_mynet/","title":"MgcNet","text":"<pre><code>%reload_ext autoreload\n%autoreload 2\nfrom pathlib import Path\nfrom mgclass import analysis, MusicGenreDataset, networks\nfrom mgclass.utils import *\nfrom mgclass.training import TrainingRun\n</code></pre> <pre><code>dry_run = False\nepochs = 10\nrepeat_count = 100\n</code></pre> <pre><code>dataset = MusicGenreDataset(\ndata_dir=Path(\"/home/georg/Music/ADL/\"),\npreprocess=create_spectrogram(win_length=2048),\nfile_transform=mp3_to_wav_location,\nplaylist_to_genre=sample_playlist_to_genre,\ntransform=create_crop((128, 128)),\ndry_run = dry_run\n)\nanalysis.summarize_dataset(dataset)\n</code></pre> <pre><code>Using genre from playlist source\nClamping dataset to 160 songs per class. Removing 279 songs.\nPreprocessing complete\n\n\n\nCreating dataset:   0%|          | 0/1280 [00:00&lt;?, ?it/s]\n\n\nmean: tensor([[[31.1943]]]), std: tensor([[[293.4358]]])\nDataset creation finished in: 108.7538 seconds\n</code></pre> <pre><code>model = networks.MgcNet(dataset.num_classes)\nrun = TrainingRun(dataset, model, batch_size=128, epochs=epochs, dry_run=dry_run, repeat_count=repeat_count)\nrun.start()\nrun.test()\nrun.plot()\n</code></pre> <pre><code>  0%|          | 0.00/10.0 [00:00&lt;?, ?epochs/s]\n\n\nStarting training for 10 epochs\nEpoch   1/10, train_loss: 2.003, train_acc: 0.265, val_loss: 1.989, val_acc: 0.266, in 93.44s\nEpoch   2/10, train_loss: 1.906, train_acc: 0.381, val_loss: 1.835, val_acc: 0.441, in 90.02s\nEpoch   3/10, train_loss: 1.812, train_acc: 0.481, val_loss: 1.769, val_acc: 0.509, in 94.53s\nEpoch   4/10, train_loss: 1.758, train_acc: 0.532, val_loss: 1.732, val_acc: 0.548, in 88.83s\nEpoch   5/10, train_loss: 1.721, train_acc: 0.566, val_loss: 1.697, val_acc: 0.579, in 91.00s\nEpoch   6/10, train_loss: 1.699, train_acc: 0.585, val_loss: 1.733, val_acc: 0.541, in 94.76s\nEpoch   7/10, train_loss: 1.679, train_acc: 0.604, val_loss: 1.690, val_acc: 0.583, in 94.91s\nEpoch   8/10, train_loss: 1.663, train_acc: 0.619, val_loss: 1.662, val_acc: 0.611, in 94.97s\nEpoch   9/10, train_loss: 1.653, train_acc: 0.628, val_loss: 1.666, val_acc: 0.607, in 93.71s\nEpoch  10/10, train_loss: 1.640, train_acc: 0.641, val_loss: 1.679, val_acc: 0.595, in 94.20s\nTraining finished in: 930.3792 seconds\n\ntest_loss: 1.614, test_acc: 0.844\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"experiments/2_resnet/","title":"2 resnet","text":"<pre><code>%reload_ext autoreload\n%autoreload 2\nfrom pathlib import Path\nfrom mgclass import analysis, MusicGenreDataset, networks\nfrom mgclass.utils import *\nfrom mgclass.training import TrainingRun\n</code></pre> <pre><code>dry_run = False\nepochs = 10\nrepeat_count = 100\n</code></pre> <pre><code>dataset = MusicGenreDataset(\ndata_dir=Path(\"/home/georg/Music/ADL/\"),\npreprocess=create_spectrogram(n_mels=128, win_length=1024),\nfile_transform=mp3_to_wav_location,\nplaylist_to_genre=sample_playlist_to_genre,\ntransform=create_crop((128, 128)),\ndry_run = dry_run,\n#max_frames=16000*60*2\n)\nanalysis.summarize_dataset(dataset)\n</code></pre> <pre><code>Using genre from playlist source\nDups for 0-6:   1\nDups for 0-7:   1\nDups for 1-4:  10\nDups for 2-7:   3\nDups for 5-7:   1\nClamping dataset to 160 songs per class. Removing 279 songs.\nPreprocessing complete\n\n\n\nCreating dataset:   0%|          | 0/1280 [00:00&lt;?, ?it/s]\n\n\nDataset creation finished in: 112.4650 seconds\n</code></pre> <pre><code>model = networks.ResNet(dataset.num_classes)\nrun = TrainingRun(dataset, model, batch_size=64, epochs=epochs, dry_run=dry_run, repeat_count=repeat_count)\nrun.start()\nrun.test()\nrun.plot()\n</code></pre> <pre><code>Starting training for 10 epoch\n\n\n\n  0%|          | 0.00/10.0 [00:00&lt;?, ?epochs/s]\n\n\nEpoch   1/10, train_loss: 1.881, train_acc: 0.395, val_loss: 1.795, val_acc: 0.473, in 68.85s\nEpoch   2/10, train_loss: 1.698, train_acc: 0.579, val_loss: 1.725, val_acc: 0.547, in 64.28s\nEpoch   3/10, train_loss: 1.627, train_acc: 0.648, val_loss: 1.694, val_acc: 0.575, in 63.67s\nEpoch   4/10, train_loss: 1.590, train_acc: 0.684, val_loss: 1.730, val_acc: 0.537, in 63.03s\nEpoch   5/10, train_loss: 1.565, train_acc: 0.709, val_loss: 1.706, val_acc: 0.564, in 63.30s\nEpoch   6/10, train_loss: 1.547, train_acc: 0.726, val_loss: 1.775, val_acc: 0.493, in 64.81s\nEpoch   7/10, train_loss: 1.531, train_acc: 0.743, val_loss: 1.745, val_acc: 0.522, in 68.35s\nEpoch   8/10, train_loss: 1.520, train_acc: 0.754, val_loss: 1.736, val_acc: 0.531, in 68.40s\nEpoch   9/10, train_loss: 1.506, train_acc: 0.768, val_loss: 1.682, val_acc: 0.586, in 67.99s\nEpoch  10/10, train_loss: 1.496, train_acc: 0.777, val_loss: 1.788, val_acc: 0.483, in 68.94s\nTraining finished in: 661.6412 seconds\ntest_loss: 1.760, test_acc: 0.594\n</code></pre> <pre><code>\n</code></pre>"},{"location":"experiments/2_resnet_decay/","title":"ResNet","text":"<pre><code>%reload_ext autoreload\n%autoreload 2\nfrom pathlib import Path\nfrom mgclass import analysis, MusicGenreDataset, networks\nfrom mgclass.utils import *\nfrom mgclass.training import TrainingRun\nimport torch.optim as optim\n</code></pre> <pre><code>dry_run = False\nepochs = 10\nrepeat_count = 100\n</code></pre> <pre><code>dataset = MusicGenreDataset(\ndata_dir=Path(\"/home/georg/Music/ADL/\"),\npreprocess=create_spectrogram(n_mels=128, win_length=1024),\nfile_transform=mp3_to_wav_location,\nplaylist_to_genre=sample_playlist_to_genre,\ntransform=create_crop((128, 128)),\ndry_run = dry_run,\n#max_frames=16000*60*2\n)\nanalysis.summarize_dataset(dataset)\n</code></pre> <pre><code>Using genre from playlist source\nDups for 0-6:   1\nDups for 0-7:   1\nDups for 1-4:  10\nDups for 2-7:   3\nDups for 5-7:   1\nClamping dataset to 160 songs per class. Removing 279 songs.\nPreprocessing complete\n\n\n\nCreating dataset:   0%|          | 0/1280 [00:00&lt;?, ?it/s]\n\n\nDataset creation finished in: 109.9891 seconds\n</code></pre> <pre><code>model = networks.ResNet(dataset.num_classes)\nrun = TrainingRun(dataset, model,\nbatch_size=64,\nepochs=epochs,\ndry_run=dry_run,\nrepeat_count=repeat_count,\noptimizer=optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005))\nrun.start()\nrun.test()\nrun.plot()\n</code></pre> <pre><code>Starting training for 10 epoch\n\n\n\n  0%|          | 0.00/10.0 [00:00&lt;?, ?epochs/s]\n\n\nEpoch   1/10, train_loss: 1.898, train_acc: 0.373, val_loss: 1.797, val_acc: 0.474, in 73.77s\nEpoch   2/10, train_loss: 1.712, train_acc: 0.567, val_loss: 1.835, val_acc: 0.427, in 69.47s\nEpoch   3/10, train_loss: 1.638, train_acc: 0.636, val_loss: 1.783, val_acc: 0.481, in 69.83s\nEpoch   4/10, train_loss: 1.601, train_acc: 0.673, val_loss: 1.733, val_acc: 0.537, in 69.38s\nEpoch   5/10, train_loss: 1.574, train_acc: 0.701, val_loss: 1.722, val_acc: 0.544, in 65.08s\nEpoch   6/10, train_loss: 1.557, train_acc: 0.717, val_loss: 1.748, val_acc: 0.518, in 66.21s\nEpoch   7/10, train_loss: 1.541, train_acc: 0.734, val_loss: 1.837, val_acc: 0.428, in 67.83s\nEpoch   8/10, train_loss: 1.526, train_acc: 0.748, val_loss: 1.738, val_acc: 0.527, in 67.07s\nEpoch   9/10, train_loss: 1.514, train_acc: 0.760, val_loss: 1.788, val_acc: 0.482, in 64.19s\nEpoch  10/10, train_loss: 1.503, train_acc: 0.771, val_loss: 1.712, val_acc: 0.556, in 63.96s\nTraining finished in: 676.8010 seconds\ntest_loss: 1.653, test_acc: 0.781\n</code></pre> <pre><code>\n</code></pre>"},{"location":"experiments/calculations/","title":"Calculations","text":"<pre><code>overlap = 0.75\nsample_rate = 16000\nbins = 128\nfor win_size in [512, 1024, 2048, 4096]:\nbin_coverage = win_size / sample_rate\ntotal_coverage = win_size * (1 + bins * (1 - overlap)) / sample_rate\nprint(f\"A win_size of {win_size:4d} covers {int(bin_coverage * 1000):4d}ms, {bins} bins cover {total_coverage:1.3f} seconds\")\n</code></pre> <pre><code>A win_size of  512 covers   32ms, 128 bins cover 1.056 seconds\nA win_size of 1024 covers   64ms, 128 bins cover 2.112 seconds\nA win_size of 2048 covers  128ms, 128 bins cover 4.224 seconds\nA win_size of 4096 covers  256ms, 128 bins cover 8.448 seconds\n</code></pre>"},{"location":"experiments/convert/","title":"Convert","text":"<pre><code>from pathlib import Path\nfrom mgclass.raw_data import convert_all\nconvert_all(Path(\"/home/georg/Music/ADL/\"))\n</code></pre> <pre><code>Converting to .wav: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2080/2080 [01:23&lt;00:00, 24.83it/s]\n</code></pre> <pre><code>\n</code></pre>"},{"location":"experiments/download/","title":"Downloading","text":"<p>A whole spotify playlist can simply be downloaded via the url.</p> <pre><code>from pathlib import Path\nfrom spotdj.spotdj import Spotdj\nimport asyncio\nplaylist_url = \"&lt;insert-playlist-url&gt;\"\nif __name__ == \"__main__\":\nwith Spotdj(rym_timeout=1) as spotdj:\nasyncio.run(spotdj.download_playlist(playlist_url))\n</code></pre>"},{"location":"experiments/final/","title":"Final","text":"<pre><code>%reload_ext autoreload\n%autoreload 2\nfrom pathlib import Path\nfrom mgclass import analysis, MusicGenreDataset, networks\nfrom mgclass.utils import *\nfrom mgclass.training import TrainingRun\nfrom torch import optim\n</code></pre> <pre><code>dry_run = False\n</code></pre> <pre><code>def run_experiment(model, win_size, batch_size=64, optimizer=None):\nif optimizer is None:\noptimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.005)\nrun = TrainingRun(dataset,\nmodel,\nepochs=100,\nbatch_size=batch_size,\ndry_run=dry_run,\nrepeat_count=100,\npatience=20,\nsave_path=Path(f\"./out/{type(model).__name__}-{win_size}.pt\"),\noptimizer=optimizer)\nrun.start(silent=True)\nrun.test()\nrun.plot(f\"{type(model).__name__} - {win_size // 16}ms resolution\", additional_info=f\"win_length: {win_size}\")\n</code></pre>"},{"location":"experiments/final/#higher-temporal-resolution","title":"Higher Temporal Resolution","text":"<pre><code>dataset = MusicGenreDataset(\ndata_dir=Path(\"/home/georg/Music/ADL/\"),\npreprocess=create_spectrogram(win_length=2048),\nfile_transform=mp3_to_wav_location,\nplaylist_to_genre=sample_playlist_to_genre,\ntransform=create_crop((128, 128)),\ndry_run = dry_run\n)\nanalysis.summarize_dataset(dataset)\n</code></pre> <pre><code>Using genre from playlist source\nClamping dataset to 160 songs per class. Removing 279 songs.\nPreprocessing complete\n\n\n\nCreating dataset:   0%|          | 0/1280 [00:00&lt;?, ?it/s]\n\n\nDataset creation finished in: 106.7265 seconds\n</code></pre>"},{"location":"experiments/final/#musicrecnet","title":"MusicRecNet","text":""},{"location":"experiments/final/#mgcnet","title":"MgcNet","text":"<pre><code>my_net = networks.MgcNet(dataset.num_classes)\nrun_experiment(my_net, batch_size=128, win_size=1024)\n</code></pre> <pre><code>Epoch   1/100:   0%|          | 0.00/100 [00:00&lt;?, ?epochs/s]\n\n\nTraining for 100 epochs\nTraining finished in: 8583.3575 seconds\n</code></pre>"},{"location":"experiments/spectrogram/","title":"Spectrogram","text":"<pre><code>%reload_ext autoreload\n%autoreload 2\nimport torchaudio\nfrom mgclass import analysis, MusicGenreDataset\nfrom pathlib import Path\nfrom mgclass.utils import *\nfrom mgclass.raw_data import download_playlists\nfrom mgclass.analysis import plot_spectrogram\n</code></pre> <pre><code>song_file = Path(\"/home/georg/Music/ADL/wav_16k/Alan Fitzpatrick - We Do What We Want (Mixed).wav\")\nspec = create_spectrogram(\nn_mels=128,\nwin_length=512\n)\nd, sample_rate = torchaudio.load(song_file, frame_offset=16000*30, num_frames=16000*5)\nd = spec(d)\nplot_spectrogram(d[0])\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"experiments/training/","title":"Full Experiment","text":"<pre><code>%reload_ext autoreload\n%autoreload 2\nfrom pathlib import Path\nfrom mgclass import analysis, MusicGenreDataset, networks\nfrom mgclass.utils import *\nfrom mgclass.training import TrainingRun\nfrom torch import optim\n</code></pre> <pre><code>dry_run = False\n</code></pre> <pre><code>def run_experiment(model, win_size, batch_size=64, optimizer=None):\nif optimizer is None:\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)\nrun = TrainingRun(dataset,\nmodel,\nepochs=100,\nbatch_size=batch_size,\ndry_run=dry_run,\nrepeat_count=100,\npatience=5,\nsave_path=Path(f\"./out/{type(model).__name__}-{win_size}.pt\"),\noptimizer=optimizer)\nrun.start(silent=True)\nrun.test()\nrun.plot(f\"{type(model).__name__} - {win_size // 16}ms resolution\", additional_info=f\"win_length: {win_size}\")\n</code></pre>"},{"location":"experiments/training/#higher-temporal-resolution","title":"Higher Temporal Resolution","text":"<pre><code>dataset = MusicGenreDataset(\ndata_dir=Path(\"/home/georg/Music/ADL/\"),\npreprocess=create_spectrogram(win_length=2048),\nfile_transform=mp3_to_wav_location,\nplaylist_to_genre=sample_playlist_to_genre,\ntransform=create_crop((128, 128)),\ndry_run = dry_run\n)\nanalysis.summarize_dataset(dataset)\n</code></pre> <pre><code>Using genre from playlist source\nClamping dataset to 160 songs per class. Removing 279 songs.\nPreprocessing complete\n\n\n\nCreating dataset:   0%|          | 0/1280 [00:00&lt;?, ?it/s]\n\n\nDataset creation finished in: 96.2477 seconds\n</code></pre>"},{"location":"experiments/training/#musicrecnet","title":"MusicRecNet","text":"<pre><code>music_rec_net = networks.MusicRecNet(dataset.num_classes)\nrun_experiment(music_rec_net, batch_size=128, win_size=1024)\n</code></pre> <pre><code>Epoch   1/100:   0%|          | 0.00/100 [00:00&lt;?, ?epochs/s]\n\n\nTraining for 100 epochs\nEarly stopped at epoch 27\nTraining finished in: 1334.8852 seconds\n</code></pre>"},{"location":"experiments/training/#resnet","title":"Resnet","text":"<pre><code>resnet18 = networks.ResNet(dataset.num_classes)\nrun_experiment(resnet18, batch_size=64, win_size=1024)\n</code></pre> <pre><code>Epoch   1/100:   0%|          | 0.00/100 [00:00&lt;?, ?epochs/s]\n\n\nTraining for 100 epochs\nEarly stopped at epoch 12\nTraining finished in: 761.6804 seconds\n</code></pre>"},{"location":"experiments/training/#mgcnet","title":"MgcNet","text":"<pre><code>my_net = networks.MgcNet(dataset.num_classes)\nrun_experiment(my_net, batch_size=128, win_size=1024)\n</code></pre> <pre><code>Epoch   1/100:   0%|          | 0.00/100 [00:00&lt;?, ?epochs/s]\n\n\nTraining for 100 epochs\nEarly stopped at epoch 13\nTraining finished in: 1115.9798 seconds\n</code></pre>"},{"location":"experiments/training/#lower-temporal-resolution","title":"Lower Temporal Resolution","text":"<pre><code>dataset = MusicGenreDataset(\ndata_dir=Path(\"/home/georg/Music/ADL/\"),\npreprocess=create_spectrogram(win_length=2048),\nfile_transform=mp3_to_wav_location,\nplaylist_to_genre=sample_playlist_to_genre,\ntransform=create_crop((128, 128)),\ndry_run = dry_run\n)\nanalysis.summarize_dataset(dataset)\n</code></pre> <pre><code>Using genre from playlist source\nClamping dataset to 160 songs per class. Removing 279 songs.\nPreprocessing complete\n\n\n\nCreating dataset:   0%|          | 0/1280 [00:00&lt;?, ?it/s]\n\n\nDataset creation finished in: 94.9764 seconds\n</code></pre>"},{"location":"experiments/training/#musicrecnet_1","title":"MusicRecNet","text":"<pre><code>music_rec_net = networks.MusicRecNet(dataset.num_classes)\nrun_experiment(music_rec_net, batch_size=128, win_size=2048)\n</code></pre> <pre><code>Epoch   1/100:   0%|          | 0.00/100 [00:00&lt;?, ?epochs/s]\n\n\nTraining for 100 epochs\nEarly stopped at epoch 24\nTraining finished in: 1170.3184 seconds\n</code></pre>"},{"location":"experiments/training/#resnet_1","title":"Resnet","text":"<pre><code>resnet18 = networks.ResNet(dataset.num_classes)\nrun_experiment(resnet18, batch_size=64, win_size=2048, optimizer=optim.SGD(resnet18.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005))\n</code></pre> <pre><code>Epoch   1/100:   0%|          | 0.00/100 [00:00&lt;?, ?epochs/s]\n\n\nTraining for 100 epochs\nEarly stopped at epoch 14\nTraining finished in: 874.6433 seconds\n</code></pre>"},{"location":"experiments/training/#mgcnet_1","title":"MgcNet","text":"<pre><code>my_net = networks.MgcNet(dataset.num_classes)\nrun_experiment(my_net, batch_size=128, win_size=2048)\n</code></pre> <pre><code>Epoch   1/100:   0%|          | 0.00/100 [00:00&lt;?, ?epochs/s]\n\n\nTraining for 100 epochs\nEarly stopped at epoch 16\nTraining finished in: 1363.0178 seconds\n</code></pre>"}]}